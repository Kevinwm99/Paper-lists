# Squeeze and Excitation networks (SE block)
**I will summarize the general idea of Squeeze and Excitation networks. You can find full text in the [orginal paper](/attention/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf)** 

 SE blocks are the set of Squeeze and Excitation operations which are use as a feature recalibration.

The SE block will improved the representation of a network over an image by investigating the interdependencies betwwen channels and the convolutional layers.

The SE block is illustrate in the Figure below

![Squeeze and Excitaion block](/attention/figures/Squeeze_n_Excitation.JPG)

If we have a transformation ![$F_{tr}: X \rightarrow U$](/equation/attention/squeeze_n_excitation/1.gif), where ![$X \in \mathbb{R}^{H^{'} \times W^{'}\times C^{'}}$](/equation/attention/squeeze_n_excitation/2.gif) and ![$U\in \mathbb{R}^{H\times W\times C}$](/equation/attention/squeeze_n_excitation/3.gif)

The feature U first will be passed through a squeeze operator to produce a channel descriptor. We can understand that this operator, for each channel, will squeeze the image into a single number. This will let the information from the global receptive field in the network to be leveraged by its lower layers. Then the excitation operation is applied, which has the self-gating mechanism. Finally the feature map U will be multiplied with the output of excitation gate to create the SE block output, and can be fed into the next layers.

A **SE networks** can be generated by simply stacking the SE blocks. Moreover the SE block can be applied any where in the architecture of a network.

Specifically, in the early layer, the informative features can will be excited by this learned SE block in order to support the quality of shared lower level representation, while in the later layers, SE block interacts with different inputs in highly class-specific.

## Notation: 
![$V=[v_1,v_2,\dots,v_C]$](/equation/attention/squeeze_n_excitation/4.gif) is the learned set of filter kernels

We can write the output of the transformation ![$F_{tr}$](/equation/attention/squeeze_n_excitation/5.gif) as ![$U=[u_1,u_2,\dots,u_C]$](/equation/attention/squeeze_n_excitation/6.gif) where

![$$ u_c=v_c*X=\sum_{s=1}^{C^{'}}v_c^s*x^s.\tag{1}$$](http://latex.codecogs.com/svg.latex?%C2%A0u_c=v_c*X=%5Csum_%7Bs=1%7D%5E%7BC%5E%7B'%7D%7Dv_c%5Es*x%5Es.)

where ![$v_c=[v_c^1,v_c^2,\dots,v_c^{C^{'}}]$](/equation/attention/squeeze_n_excitation/7.gif) and ![$X=[x^1,x^2,\dots ,x^{C{'}}]$](/equation/attention/squeeze_n_excitation/8.gif)

![$v_c^s$](/equation/attention/squeeze_n_excitation/9.gif) is a 2D kernel: represent a single channel of ![$v_c$](/equation/attention/squeeze_n_excitation/10.gif) which coresponds to the channel of X. Since the out put is produced by a summation through all channels, the channel dependencies is in ![$v_c$](/equation/attention/squeeze_n_excitation/10.gif)

## Squeeze Operator: Global Information Embedding

- This operator is straight forward, just by simpling stretching out the width and height of the image for each channel.

![$$z_c = F_{sq}(u_c)=\frac{1}{H\times W}\sum_{i=1}^{H}\sum_{j=1}^{W}u_c(i,j)\tag{2}$$](/equation/attention/squeeze_n_excitation/12.gif)

## Excitation Operator: Adaptive Recalibration 
- This operator will fully capture channel-wise dependencies 
- It is a gating mechanism with sigmoid

![$$ s=F_{ex}(z,W)=\sigma(g(z,W)) = \sigma(W_2\delta(W_1z))\tag{3} $$](/equation/attention/squeeze_n_excitation/13.gif)

where ![$\delta$](/equation/attention/squeeze_n_excitation/14.gif) is the ReLU function, ![$W_1\in\mathbb{R}^{\frac{C}{r}\times C}$](/equation/attention/squeeze_n_excitation/15.gif) and ![$W_2\in\mathbb{R}^{ C\times\frac{C}{r}}$](/equation/attention/squeeze_n_excitation/16.gif)

![$W_1$](/equation/attention/squeeze_n_excitation/17.gif) serves as a dimensionality-reduction with reduction ratio ![$r$](/equation/attention/squeeze_n_excitation/18.gif), while ![$W_2$](/equation/attention/squeeze_n_excitation/19.gif) serves as a dimensionality-incresing.

The final output of SE block is then multiplied with the transformation output.
![$$ \tilde{x_c} =F_{scale}(u_c,s_c) =s_c . u_c \tag{4} $$](/equation/attention/squeeze_n_excitation/20.gif)

where ![$\tilde{X}=[\tilde{x_1},\tilde{x_2},\dots,\tilde{x_C}]$](/equation/attention/squeeze_n_excitation/21.gif) and ![$F_{scale}(u_c,s_c)$](/equation/attention/squeeze_n_excitation/22.gif) refers to channel-wise multiplication between the feature map ![$u_c\in \mathbb{R}^{H\times W}$](/equation/attention/squeeze_n_excitation/23.gif) and the scalar ![$s_c$](/equation/attention/squeeze_n_excitation/24.gif)

The reduction ratio ![$r$](/equation/attention/squeeze_n_excitation/18.gif) is a crucial hyperparameter which affects directly to the capacity and computational cost of the SE blocks. The table below shows that the performance does not increase with increased capacity. The author states that ![$r=16$](/equation/attention/squeeze_n_excitation/25.gif) achieve a good traceoff in accuracy and complexity. Note that the experiment is based on SE-ResNet-50

![Reduction ratio](/attention/figures/ratio.JPG)

## Some SE architectures:

Inception and SE-Inception | ResNet and SE-ResNet
-|-
![Se_inception](/attention/figures/seinception.JPG)| ![Se_resnet](/attention/figures/seresnet.JPG)

## Computational Cost
![computational cost](/attention/figures/cost.JPG)
## Performance
![performance](/attention/figures/perform.JPG)